{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulation des données\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "# Gestion des fichiers et des chemins\n",
    "import glob\n",
    "\n",
    "# Prétraitement des images (TensorFlow / Keras)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Prétraitement des étiquettes\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\mauge\\Openclassrooms\\Projet 7 - Git repository\\Veille\\Data\\flipkart_com-ecommerce_sample_1050.csv\")\n",
    "\n",
    "# Récupération des catégories\n",
    "def extract_first_category(category_string):\n",
    "    # Convertir la chaîne de caractères en liste\n",
    "    category_list = ast.literal_eval(category_string)\n",
    "    \n",
    "    # Sélectionner la première valeur de la liste\n",
    "    full_category = category_list[0]\n",
    "    \n",
    "    # Sélectionner la première partie avant '>>'\n",
    "    first_category = full_category.split('>>')[0].strip()\n",
    "    \n",
    "    return first_category\n",
    "\n",
    "df['main_category'] = df['product_category_tree'].apply(extract_first_category)\n",
    "list_labels = df['main_category'].unique().tolist()\n",
    "df.head(1)\n",
    "df.shape\n",
    "list_labels\n",
    "\n",
    "\n",
    "path = r\"C:\\Users\\mauge\\Openclassrooms\\Projet 7 - Git repository\\Veille\\Data\\Images\"\n",
    "\n",
    "data_path = glob.glob(path + '/*.jp*')\n",
    "\n",
    "def data_fct(path) :\n",
    "    list_photos = [file for file in path]\n",
    "    print(len(list_photos))\n",
    "    data = pd.DataFrame()\n",
    "    data[\"image_path\"] = list_photos\n",
    "    return data\n",
    "\n",
    "data = data_fct(data_path)\n",
    "\n",
    "data.head(5)\n",
    "data['file_id'] = data['image_path'].apply(lambda x: x.split('\\\\')[-1].split('.')[0])\n",
    "df.rename(columns={'uniq_id': 'file_id'}, inplace=True)\n",
    "data = pd.merge(data, df, on='file_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>file_id</th>\n",
       "      <th>crawl_timestamp</th>\n",
       "      <th>product_url</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_category_tree</th>\n",
       "      <th>pid</th>\n",
       "      <th>retail_price</th>\n",
       "      <th>discounted_price</th>\n",
       "      <th>image</th>\n",
       "      <th>is_FK_Advantage_product</th>\n",
       "      <th>description</th>\n",
       "      <th>product_rating</th>\n",
       "      <th>overall_rating</th>\n",
       "      <th>brand</th>\n",
       "      <th>product_specifications</th>\n",
       "      <th>main_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\mauge\\Openclassrooms\\Projet 7 - Git r...</td>\n",
       "      <td>009099b1f6e1e8f893ec29a7023153c4</td>\n",
       "      <td>2016-04-24 18:34:50 +0000</td>\n",
       "      <td>http://www.flipkart.com/palito-plo-166-analog-...</td>\n",
       "      <td>palito PLO 166 Analog Watch  - For Girls, Women</td>\n",
       "      <td>[\"Watches &gt;&gt; Wrist Watches &gt;&gt; palito Wrist Wat...</td>\n",
       "      <td>WATEHZPFAPQKUASQ</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>009099b1f6e1e8f893ec29a7023153c4.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>Specifications of palito PLO 166 Analog Watch ...</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>palito</td>\n",
       "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Type\", \"va...</td>\n",
       "      <td>Watches</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path  \\\n",
       "0  C:\\Users\\mauge\\Openclassrooms\\Projet 7 - Git r...   \n",
       "\n",
       "                            file_id            crawl_timestamp  \\\n",
       "0  009099b1f6e1e8f893ec29a7023153c4  2016-04-24 18:34:50 +0000   \n",
       "\n",
       "                                         product_url  \\\n",
       "0  http://www.flipkart.com/palito-plo-166-analog-...   \n",
       "\n",
       "                                      product_name  \\\n",
       "0  palito PLO 166 Analog Watch  - For Girls, Women   \n",
       "\n",
       "                               product_category_tree               pid  \\\n",
       "0  [\"Watches >> Wrist Watches >> palito Wrist Wat...  WATEHZPFAPQKUASQ   \n",
       "\n",
       "   retail_price  discounted_price                                 image  \\\n",
       "0        1500.0             199.0  009099b1f6e1e8f893ec29a7023153c4.jpg   \n",
       "\n",
       "   is_FK_Advantage_product                                        description  \\\n",
       "0                    False  Specifications of palito PLO 166 Analog Watch ...   \n",
       "\n",
       "        product_rating       overall_rating   brand  \\\n",
       "0  No rating available  No rating available  palito   \n",
       "\n",
       "                              product_specifications main_category  \n",
       "0  {\"product_specification\"=>[{\"key\"=>\"Type\", \"va...       Watches  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWIN Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoImageProcessor, SwinForImageClassification\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_encoded\n",
       "6    150\n",
       "5    150\n",
       "4    150\n",
       "1    150\n",
       "2    150\n",
       "3    150\n",
       "0    150\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialiser le LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Ajouter une colonne encodée\n",
    "data['label_encoded'] = label_encoder.fit_transform(data['main_category'])\n",
    "\n",
    "data['label_encoded'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>file_id</th>\n",
       "      <th>crawl_timestamp</th>\n",
       "      <th>product_url</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_category_tree</th>\n",
       "      <th>pid</th>\n",
       "      <th>retail_price</th>\n",
       "      <th>discounted_price</th>\n",
       "      <th>image</th>\n",
       "      <th>is_FK_Advantage_product</th>\n",
       "      <th>description</th>\n",
       "      <th>product_rating</th>\n",
       "      <th>overall_rating</th>\n",
       "      <th>brand</th>\n",
       "      <th>product_specifications</th>\n",
       "      <th>main_category</th>\n",
       "      <th>label_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\mauge\\Openclassrooms\\Projet 7 - Git r...</td>\n",
       "      <td>009099b1f6e1e8f893ec29a7023153c4</td>\n",
       "      <td>2016-04-24 18:34:50 +0000</td>\n",
       "      <td>http://www.flipkart.com/palito-plo-166-analog-...</td>\n",
       "      <td>palito PLO 166 Analog Watch  - For Girls, Women</td>\n",
       "      <td>[\"Watches &gt;&gt; Wrist Watches &gt;&gt; palito Wrist Wat...</td>\n",
       "      <td>WATEHZPFAPQKUASQ</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>009099b1f6e1e8f893ec29a7023153c4.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>Specifications of palito PLO 166 Analog Watch ...</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>palito</td>\n",
       "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Type\", \"va...</td>\n",
       "      <td>Watches</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path  \\\n",
       "0  C:\\Users\\mauge\\Openclassrooms\\Projet 7 - Git r...   \n",
       "\n",
       "                            file_id            crawl_timestamp  \\\n",
       "0  009099b1f6e1e8f893ec29a7023153c4  2016-04-24 18:34:50 +0000   \n",
       "\n",
       "                                         product_url  \\\n",
       "0  http://www.flipkart.com/palito-plo-166-analog-...   \n",
       "\n",
       "                                      product_name  \\\n",
       "0  palito PLO 166 Analog Watch  - For Girls, Women   \n",
       "\n",
       "                               product_category_tree               pid  \\\n",
       "0  [\"Watches >> Wrist Watches >> palito Wrist Wat...  WATEHZPFAPQKUASQ   \n",
       "\n",
       "   retail_price  discounted_price                                 image  \\\n",
       "0        1500.0             199.0  009099b1f6e1e8f893ec29a7023153c4.jpg   \n",
       "\n",
       "   is_FK_Advantage_product                                        description  \\\n",
       "0                    False  Specifications of palito PLO 166 Analog Watch ...   \n",
       "\n",
       "        product_rating       overall_rating   brand  \\\n",
       "0  No rating available  No rating available  palito   \n",
       "\n",
       "                              product_specifications main_category  \\\n",
       "0  {\"product_specification\"=>[{\"key\"=>\"Type\", \"va...       Watches   \n",
       "\n",
       "   label_encoded  \n",
       "0              6  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'images pour l'entraînement : 840\n",
      "Nombre d'images pour la validation : 210\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split en train et validation\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, stratify=data['main_category'], random_state=42)\n",
    "\n",
    "print(f\"Nombre d'images pour l'entraînement : {len(train_data)}\")\n",
    "print(f\"Nombre d'images pour la validation : {len(val_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor):\n",
    "        self.dataframe = dataframe\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.dataframe.iloc[idx]['image_path']\n",
    "        label = self.dataframe.iloc[idx]['label_encoded']\n",
    "\n",
    "        # Charger et redimensionner l'image\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        img = img.resize((224, 224))  # Taille standard pour Swin Transformer\n",
    "\n",
    "        # Prétraitement\n",
    "        processed = self.processor(images=img, return_tensors=\"pt\")\n",
    "\n",
    "        return processed['pixel_values'].squeeze(0), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le processeur pour Swin Transformer\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
    "\n",
    "# Création des datasets\n",
    "train_dataset = CustomDataset(train_data, processor)\n",
    "val_dataset = CustomDataset(val_data, processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels originaux : ['Watches' 'Kitchen & Dining' 'Home Furnishing' 'Beauty and Personal Care'\n",
      " 'Computers' 'Home Decor & Festive Needs' 'Baby Care']\n",
      "Labels encodés : [6 5 4 1 2 3 0]\n"
     ]
    }
   ],
   "source": [
    "# Vérifier les labels encodés\n",
    "print(\"Labels originaux :\", data['main_category'].unique())\n",
    "print(\"Labels encodés :\", label_encoder.transform(data['main_category'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([32, 3, 224, 224])\n",
      "Label batch shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(f\"Image batch shape: {images.shape}\")\n",
    "    print(f\"Label batch shape: {labels.shape}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de classes : 7\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Création des DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Vérification des classes\n",
    "num_classes = len(train_data['main_category'].unique())\n",
    "print(\"Nombre de classes :\", num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinForImageClassification(\n",
       "  (swin): SwinModel(\n",
       "    (embeddings): SwinEmbeddings(\n",
       "      (patch_embeddings): SwinPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      )\n",
       "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): SwinEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): SwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (key): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (value): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (key): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (value): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.00909090880304575)\n",
       "              (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): SwinPatchMerging(\n",
       "            (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.0181818176060915)\n",
       "              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.027272727340459824)\n",
       "              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): SwinPatchMerging(\n",
       "            (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): SwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.036363635212183)\n",
       "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.045454543083906174)\n",
       "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.054545458406209946)\n",
       "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (3): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.06363636255264282)\n",
       "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (4): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.0727272778749466)\n",
       "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (5): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.08181818574666977)\n",
       "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): SwinPatchMerging(\n",
       "            (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): SwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.09090909361839294)\n",
       "              (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.10000000149011612)\n",
       "              (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (pooler): AdaptiveAvgPool1d(output_size=1)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import SwinForImageClassification\n",
    "import torch.nn as nn\n",
    "\n",
    "# Charger le modèle pré-entraîné avec gestion des tailles incompatibles\n",
    "model_name = \"microsoft/swin-tiny-patch4-window7-224\"\n",
    "model = SwinForImageClassification.from_pretrained(\n",
    "    model_name,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Remplacer la couche de classification\n",
    "num_classes = 7  # Nombre de classes de votre tâche\n",
    "model.classifier = nn.Linear(in_features=model.classifier.in_features, out_features=num_classes)\n",
    "\n",
    "# Déplacer le modèle sur le GPU ou CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Définition de l'optimiseur et de la fonction de perte\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)\n",
    "loss_fn = CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, loss_fn, device):\n",
    "    model.eval()  # Mettre le modèle en mode évaluation\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # Pas de calcul des gradients en validation\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images).logits\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_loss = total_loss / len(val_loader)\n",
    "    val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return val_loss, val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score  # Pour calculer l'accuracy\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, loss_fn, device, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).logits\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Afficher les logs tous les 10 batches\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f\"Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_accuracy = validate_model(model, val_loader, loss_fn, device)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Batch 10/27, Loss: 1.7559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mauge\\Openclassrooms\\Projet 7 - Git repository\\gradio_env\\Lib\\site-packages\\PIL\\Image.py:3406: DecompressionBombWarning: Image size (93680328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20/27, Loss: 1.3872\n",
      "Train Loss: 1.6206, Train Accuracy: 0.4619, Val Loss: 1.1872, Val Accuracy: 0.7095\n",
      "Epoch 2/5\n",
      "Batch 10/27, Loss: 0.8169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mauge\\Openclassrooms\\Projet 7 - Git repository\\gradio_env\\Lib\\site-packages\\PIL\\Image.py:3406: DecompressionBombWarning: Image size (93680328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20/27, Loss: 0.6426\n",
      "Train Loss: 0.7047, Train Accuracy: 0.8548, Val Loss: 0.6768, Val Accuracy: 0.7762\n",
      "Epoch 3/5\n",
      "Batch 10/27, Loss: 0.4175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mauge\\Openclassrooms\\Projet 7 - Git repository\\gradio_env\\Lib\\site-packages\\PIL\\Image.py:3406: DecompressionBombWarning: Image size (93680328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20/27, Loss: 0.1243\n",
      "Train Loss: 0.3277, Train Accuracy: 0.9155, Val Loss: 0.6126, Val Accuracy: 0.8048\n",
      "Epoch 4/5\n",
      "Batch 10/27, Loss: 0.2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mauge\\Openclassrooms\\Projet 7 - Git repository\\gradio_env\\Lib\\site-packages\\PIL\\Image.py:3406: DecompressionBombWarning: Image size (93680328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20/27, Loss: 0.1261\n",
      "Train Loss: 0.1734, Train Accuracy: 0.9655, Val Loss: 0.6184, Val Accuracy: 0.8190\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mauge\\Openclassrooms\\Projet 7 - Git repository\\gradio_env\\Lib\\site-packages\\PIL\\Image.py:3406: DecompressionBombWarning: Image size (93680328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10/27, Loss: 0.0904\n",
      "Batch 20/27, Loss: 0.0514\n",
      "Train Loss: 0.0943, Train Accuracy: 0.9869, Val Loss: 0.6189, Val Accuracy: 0.8095\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, optimizer, loss_fn, device, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Prédictions\n",
    "            outputs = model(images).logits\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "evaluate_model(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le modèle\n",
    "model.save_pretrained(\"swin_model\")\n",
    "\n",
    "# Recharger le modèle\n",
    "model = SwinForImageClassification.from_pretrained(\"swin_model\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoImageProcessor, ViTForImageClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_encoded\n",
       "6    150\n",
       "5    150\n",
       "4    150\n",
       "1    150\n",
       "2    150\n",
       "3    150\n",
       "0    150\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialiser le LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Ajouter une colonne encodée\n",
    "data['label_encoded'] = label_encoder.fit_transform(data['main_category'])\n",
    "\n",
    "data['label_encoded'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'images pour l'entraînement : 840\n",
      "Nombre d'images pour la validation : 210\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split en train et validation\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, stratify=data['main_category'], random_state=42)\n",
    "\n",
    "print(f\"Nombre d'images pour l'entraînement : {len(train_data)}\")\n",
    "print(f\"Nombre d'images pour la validation : {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor):\n",
    "        self.dataframe = dataframe\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Charger l'image\n",
    "        image_path = self.dataframe.iloc[idx]['image_path']\n",
    "        label = self.dataframe.iloc[idx]['label_encoded']\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Prétraiter l'image\n",
    "        processed = self.processor(images=img, return_tensors=\"pt\")\n",
    "        return processed['pixel_values'].squeeze(0), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, ViTForImageClassification\n",
    "\n",
    "# Charger le processeur et le modèle\n",
    "model_name = \"google/vit-base-patch16-224\"\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_classes\n",
    ")\n",
    "\n",
    "# Déplacer le modèle sur le GPU ou CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer les datasets\n",
    "train_dataset = CustomDataset(train_data, processor)\n",
    "val_dataset = CustomDataset(val_data, processor)\n",
    "\n",
    "# Créer les DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, loss_fn, device, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).logits\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        train_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
